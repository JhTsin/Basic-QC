{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import idx2numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        # 初始化Adam优化器的参数\n",
    "        self.learning_rate = learning_rate  # 设置学习率\n",
    "        self.beta1 = beta1  # 设置一阶矩估计（momentum）的衰减率\n",
    "        self.beta2 = beta2  # 设置二阶矩估计（RMSprop）的衰减率\n",
    "        self.epsilon = epsilon  # 防止除以零的小常数\n",
    "        self.t = 0  # 初始化时间步，用于偏差校正\n",
    "\n",
    "    def update_parameters(self, nn, gradients_w, gradients_b):\n",
    "        # 更新神经网络的参数\n",
    "        if not hasattr(self, 'm_w'):\n",
    "            # 如果moment向量还没有初始化，则进行初始化\n",
    "            self.m_w = [np.zeros_like(w) for w in nn.weights]  # 初始化权重的一阶矩估计\n",
    "            self.v_w = [np.zeros_like(w) for w in nn.weights]  # 初始化权重的二阶矩估计\n",
    "            self.m_b = [np.zeros_like(b) for b in nn.biases]   # 初始化偏置的一阶矩估计\n",
    "            self.v_b = [np.zeros_like(b) for b in nn.biases]   # 初始化偏置的二阶矩估计\n",
    "\n",
    "        self.t += 1  # 增加时间步\n",
    "        correction1 = 1 - self.beta1 ** self.t  # 计算一阶矩估计的偏差校正因子\n",
    "        correction2 = 1 - self.beta2 ** self.t  # 计算二阶矩估计的偏差校正因子\n",
    "\n",
    "        for i in range(len(nn.weights)):\n",
    "            # 遍历神经网络的每一层\n",
    "            self.m_w[i] *= self.beta1  # 对权重的一阶矩估计应用指数衰减\n",
    "            self.m_w[i] += (1 - self.beta1) * gradients_w[i]  # 更新权重的一阶矩估计\n",
    "            self.v_w[i] *= self.beta2  # 对权重的二阶矩估计应用指数衰减\n",
    "            self.v_w[i] += (1 - self.beta2) * np.square(gradients_w[i])  # 更新权重的二阶矩估计\n",
    "\n",
    "            m_w_corrected = self.m_w[i] / correction1  # 应用偏差校正到权重的一阶矩估计\n",
    "            v_w_corrected = self.v_w[i] / correction2  # 应用偏差校正到权重的二阶矩估计\n",
    "\n",
    "            nn.weights[i] -= self.learning_rate * m_w_corrected / (np.sqrt(v_w_corrected) + self.epsilon)  # 更新权重\n",
    "\n",
    "            self.m_b[i] *= self.beta1  # 对偏置的一阶矩估计应用指数衰减\n",
    "            self.m_b[i] += (1 - self.beta1) * gradients_b[i]  # 更新偏置的一阶矩估计\n",
    "            self.v_b[i] *= self.beta2  # 对偏置的二阶矩估计应用指数衰减\n",
    "            self.v_b[i] += (1 - self.beta2) * np.square(gradients_b[i])  # 更新偏置的二阶矩估计\n",
    "\n",
    "            m_b_corrected = self.m_b[i] / correction1  # 应用偏差校正到偏置的一阶矩估计\n",
    "            v_b_corrected = self.v_b[i] / correction2  # 应用偏差校正到偏置的二阶矩估计\n",
    "\n",
    "            nn.biases[i] -= self.learning_rate * m_b_corrected / (np.sqrt(v_b_corrected) + self.epsilon)  # 更新偏置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate, batch_size):\n",
    "        # 神经网络的构造函数\n",
    "        #np.random.seed(1)  # 设置随机种子以确保结果的可重复性\n",
    "        # 初始化网络的权重，使用标准正态分布随机生成\n",
    "        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i + 1]) for i in range(len(layer_sizes) - 1)]\n",
    "        # 初始化网络的偏置，全部设置为零\n",
    "        self.biases = [np.zeros((1, layer_sizes[i + 1])) for i in range(len(layer_sizes) - 1)]\n",
    "        # 创建一个 Adam 优化器实例用于训练\n",
    "        self.optimizer = AdamOptimizer(learning_rate=learning_rate)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        # Sigmoid 激活函数\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        # Sigmoid 函数的导数\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # Softmax 函数，用于多分类问题的输出层\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # 防止数值溢出\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def fp(self, input):\n",
    "        # 神经网络的前向传播\n",
    "        activations = [input]  # 存储每层的激活值\n",
    "        # 遍历除最后一层外的每一层\n",
    "        for w, b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            # 计算下一层的激活值\n",
    "            activations.append(self.sigmoid(np.dot(activations[-1], w) + b))\n",
    "        # 计算最后一层的激活值，使用 softmax\n",
    "        activations.append(self.softmax(np.dot(activations[-1], self.weights[-1]) + self.biases[-1]))\n",
    "        return activations  # 返回所有层的激活值\n",
    "\n",
    "\n",
    "    def bp(self, x, y):\n",
    "        # 神经网络的反向传播\n",
    "        activations = self.fp(x)  # 获取前向传播的结果\n",
    "        dL_dy = activations[-1] - y  # 计算输出层的误差的偏导数\n",
    "        gradients_w = []  # 存储权重的梯度\n",
    "        gradients_b = []  # 存储偏置的梯度\n",
    "\n",
    "        # 从输出层到输入层反向遍历每一层\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            # 计算激活值对加权输入的偏导数\n",
    "            da_dz = self.sigmoid_derivative(activations[i+1])\n",
    "            # 应用链式法则更新偏导数\n",
    "            if i != len(self.weights) - 1:\n",
    "                dL_dz = dL_dy * da_dz\n",
    "            else:\n",
    "                dL_dz = dL_dy  # 对于输出层，偏导数就是dL_dy\n",
    "            grad_w = activations[i].T.dot(dL_dz)  # 计算权重的梯度\n",
    "            grad_b = np.sum(dL_dz, axis=0, keepdims=True)  # 计算偏置的梯度\n",
    "            gradients_w.append(grad_w)  # 添加权重梯度\n",
    "            gradients_b.append(grad_b)  # 添加偏置梯度\n",
    "            # 更新偏导数传播到前一层\n",
    "            if i != 0:\n",
    "                dL_dy = dL_dz.dot(self.weights[i].T)\n",
    "\n",
    "        gradients_w.reverse()  # 反转梯度列表\n",
    "        gradients_b.reverse()  # 反转梯度列表\n",
    "\n",
    "        # 计算分类正确性\n",
    "        predictions = self.predict(x)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(y, axis=1)\n",
    "        samples_tf_bp = predicted_classes != true_classes  # 标记分类正确与错误的样本\n",
    "\n",
    "        return gradients_w, gradients_b, samples_tf_bp  # 返回梯度和分类正确与错误的样本信息\n",
    "\n",
    "\n",
    "    # def train(self, x, y, update_weights=True):\n",
    "    #     # 训练神经网络\n",
    "    #     losses = []  # 用于存储每批次的损失\n",
    "    #     num_batches = x.shape[0] // self.batch_size\n",
    "    #     samples_ft = np.ones(x.shape[0], dtype=bool)  # 初始化为所有样本都分类正确\n",
    "\n",
    "    #     with tqdm(total=num_batches, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "    #         for i in range(0, x.shape[0], self.batch_size):\n",
    "    #             x_batch = x[i:i + self.batch_size]  # 获取一批训练数据\n",
    "    #             y_batch = y[i:i + self.batch_size]  # 获取一批训练标签\n",
    "    #             gradients_w, gradients_b, batch_errors = self.bp(x_batch, y_batch)  # 执行反向传播\n",
    "\n",
    "    #             if update_weights:\n",
    "    #                 self.optimizer.update_parameters(self, gradients_w, gradients_b)  # 更新权重和偏置\n",
    "\n",
    "    #             samples_ft[i:i + self.batch_size] = ~batch_errors  # 更新该批次的样本分类正确性\n",
    "\n",
    "    #             y_pred = self.predict(x_batch)  # 进行预测\n",
    "    #             loss = self.cross_entropy_loss(y_pred, y_batch)  # 计算损失\n",
    "    #             losses.append(loss)  # 添加损失\n",
    "\n",
    "    #             pbar.update(1)\n",
    "    #             pbar.set_postfix(loss=np.mean(losses))  # 显示当前批次的损失\n",
    "\n",
    "    #     return samples_ft  # 返回分类正确的样本信息\n",
    "    \n",
    "    def train(self, x, y, epochs, update_weights=True):\n",
    "        # 训练神经网络\n",
    "        num_batches = x.shape[0] // self.batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            losses = []  # 用于存储每批次的损失\n",
    "            samples_ft = np.ones(x.shape[0], dtype=bool)  # 初始化为所有样本都分类正确\n",
    "\n",
    "            with tqdm(total=num_batches, desc=\"Training\", unit=\"batch\") as pbar:\n",
    "                for i in range(0, x.shape[0], self.batch_size):\n",
    "                    x_batch = x[i:i + self.batch_size]  # 获取一批训练数据\n",
    "                    y_batch = y[i:i + self.batch_size]  # 获取一批训练标签\n",
    "                    gradients_w, gradients_b, batch_errors = self.bp(x_batch, y_batch)  # 执行反向传播\n",
    "\n",
    "                    if update_weights:\n",
    "                        self.optimizer.update_parameters(self, gradients_w, gradients_b)  # 更新权重和偏置\n",
    "\n",
    "                    samples_ft[i:i + self.batch_size] = ~batch_errors  # 更新该批次的样本分类正确性\n",
    "\n",
    "                    y_pred = self.predict(x_batch)  # 进行预测\n",
    "                    loss = self.cross_entropy_loss(y_pred, y_batch)  # 计算损失\n",
    "                    losses.append(loss)  # 添加损失\n",
    "\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix(loss=np.mean(losses))  # 显示当前批次的平均损失\n",
    "\n",
    "            # 可以在此处添加代码来评估在验证集上的表现，如果有的话\n",
    "\n",
    "        return samples_ft  # 返回分类正确的样本信息\n",
    "\n",
    "\n",
    "    def cross_entropy_loss(self, y_pred, y_true):\n",
    "        # 交叉熵损失函数\n",
    "        m = y_true.shape[0]  # 获取样本数量\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m  # 计算交叉熵损失\n",
    "        return loss\n",
    "\n",
    "    def predict(self, input):\n",
    "        # 预测函数\n",
    "        return self.fp(input)[-1]  # 返回最后一层的激活值，即预测结果\n",
    "\n",
    "    def evaluate_accuracy(self, x, y):\n",
    "        # 评估模型准确率\n",
    "        predictions = self.predict(x)  # 获取预测结果\n",
    "        predicted_classes = np.argmax(predictions, axis=1)  # 获取预测的类别\n",
    "        true_classes = np.argmax(y, axis=1)  # 获取真实的类别\n",
    "        return np.mean(predicted_classes == true_classes)  # 计算准确率\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    # 加载 MNIST 数据集\n",
    "    train_images = idx2numpy.convert_from_file(r\"..\\train-images.idx3-ubyte\")\n",
    "    train_labels = idx2numpy.convert_from_file(r\"..\\train-labels.idx1-ubyte\")\n",
    "    test_images = idx2numpy.convert_from_file(r\"..\\t10k-images.idx3-ubyte\")\n",
    "    test_labels = idx2numpy.convert_from_file(r\"..\\t10k-labels.idx1-ubyte\")\n",
    "\n",
    "    # 将图像数据转换为一维数组并归一化\n",
    "    train_images = train_images.reshape((train_images.shape[0], -1)).astype('float32') / 255\n",
    "    test_images = test_images.reshape((test_images.shape[0], -1)).astype('float32') / 255\n",
    "\n",
    "    # 将标签转换为 one-hot 编码\n",
    "    train_labels = one_hot_encode(train_labels)\n",
    "    test_labels = one_hot_encode(test_labels)\n",
    "\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    # 将类别标签转换为 one-hot 编码格式\n",
    "    one_hot_labels = np.zeros((labels.shape[0], num_classes))\n",
    "    one_hot_labels[np.arange(labels.shape[0]), labels] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def weighted_resampling(data, labels, probabilities, num_samples=None):\n",
    "    if num_samples is None:\n",
    "        num_samples = len(data)\n",
    "\n",
    "    # 根据概率进行加权随机抽样\n",
    "    sample_indices = np.random.choice(np.arange(len(data)), size=num_samples, p=probabilities)\n",
    "\n",
    "    # 根据抽取的索引创建重采样后的数据集\n",
    "    resampled_data = data[sample_indices]\n",
    "    resampled_labels = labels[sample_indices]\n",
    "\n",
    "    return resampled_data, resampled_labels\n",
    "\n",
    "def train_nn(train_images, train_labels, weights):\n",
    "    train_accuracies = []  # 用于存储每轮训练的准确率\n",
    "    test_accuracies = []   # 用于存储每轮测试的准确率\n",
    "    classifiers = []\n",
    "    num_correct = []\n",
    "    num_incorrect = []\n",
    "    epsilon = []\n",
    "    alpha = []\n",
    "\n",
    "    resampled_train_images, resampled_train_labels = weighted_resampling(train_images, train_labels, weights)\n",
    "    samples_ft = nn.train(resampled_train_images, resampled_train_labels, epochs=epoch_num)\n",
    "    classifiers.append(nn)\n",
    "\n",
    "    num_correct.append(np.count_nonzero(samples_ft))\n",
    "    num_incorrect.append(len(samples_ft) - num_correct[-1])\n",
    "    print('错误样本个数: ', num_incorrect[-1])\n",
    "\n",
    "    train_accuracy = nn.evaluate_accuracy(resampled_train_images, resampled_train_labels)  # 计算训练准确率\n",
    "    test_accuracy = nn.evaluate_accuracy(test_images, test_labels)  # 计算测试准确率\n",
    "    train_accuracies.append(train_accuracy)  # 记录训练准确率\n",
    "    test_accuracies.append(test_accuracy)  # 记录测试准确率\n",
    "        \n",
    "    epsilon.append(1 - np.sum(weights[samples_ft]))\n",
    "    # print(f'当前错误率: {epsilon[-1]:.4f}')\n",
    "        \n",
    "    alpha.append(0.5 * np.log((2.5 - epsilon[-1]) / epsilon[-1]))\n",
    "    samples_ft = np.where(samples_ft, 1, -1)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        weights[i] = weights[i]*np.exp(-alpha[-1]*samples_ft[i])\n",
    "    weights = weights / np.sum(weights)\n",
    "    # print(f'当前分类器输出的权重: {weights}')\n",
    "\n",
    "    return weights, train_accuracies, test_accuracies, classifiers, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 MNIST 数据集\n",
    "train_images, train_labels, test_images, test_labels = load_mnist()\n",
    "\n",
    "# 显示总样本数，训练集和测试集的大小\n",
    "total_samples = train_images.shape[0] + test_images.shape[0]\n",
    "train_size = train_images.shape[0]\n",
    "test_size = test_images.shape[0]\n",
    "\n",
    "print(f\"总样本数: {total_samples}\", f\"训练集大小: {train_size}\", f\"测试集大小: {test_size}\")\n",
    "\n",
    "\n",
    "num_samples = len(train_images)\n",
    "weights_0 = np.ones(num_samples) / num_samples  # 初始化样本权重\n",
    "\n",
    "# 使用函数进行重采样\n",
    "\n",
    "epoch_num = 5\n",
    "learning_rate = 0.05\n",
    "batch_size = 60000\n",
    "layer_sizes = [784,15,10]\n",
    "\n",
    "print(f\"学习率: {learning_rate}, batch_size: {batch_size}\")\n",
    "# 创建神经网络实例\n",
    "\n",
    "\n",
    "\n",
    "# 显示隐藏层的层数以及每层神经元的个数\n",
    "num_hidden_layers = len(layer_sizes) - 2  # 输入层和输出层之外的层数\n",
    "print(f\"隐藏层的层数: {num_hidden_layers}\")\n",
    "for i, size in enumerate(layer_sizes[1:-1], start=1):\n",
    "    print(f\"隐藏层 {i} 的神经元个数: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nn = SimpleNeuralNetwork(layer_sizes, learning_rate=learning_rate, batch_size= batch_size)\n",
    "\n",
    "num_classifier = 10  # 分类器的个数\n",
    "weights =  np.ones(num_samples) / num_samples\n",
    "\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "final_accuracy_test_list = []\n",
    "final_accuracy_train_list = []\n",
    "combined_output_test = np.zeros((len(test_images), 10))\n",
    "combined_output_train = np.zeros((len(train_images), 10))\n",
    "alpha_test = []\n",
    "for i in range(num_classifier):\n",
    "    print(f'-------------------第{i+1}个分类器-------------------')\n",
    "    weights, train_acc, test_acc, classifiers, alpha = train_nn(train_images, train_labels, weights)\n",
    "    train_accuracy.append(train_acc)\n",
    "    test_accuracy.append(test_acc)\n",
    "    alpha_test.append(alpha)\n",
    "\n",
    "    #在测试集上进行测试\n",
    "    predictions_test = classifiers[-1].predict(test_images)  # 获取预测结果\n",
    "    combined_output_test += alpha * predictions_test   # 计算组合输出\n",
    "    final_predictions_test = np.argmax(combined_output_test, axis=1)  # 获取组合预测的类别\n",
    "    true_label_test = np.argmax(test_labels, axis=1)    # 获取真实的类别\n",
    "    final_accuracy_test = np.mean(final_predictions_test == true_label_test) # 计算准确率\n",
    "    final_accuracy_test_list.append(final_accuracy_test)  # 记录训练准确率\n",
    "\n",
    "\n",
    "    #在训练集上进行测试\n",
    "    predictions_train = classifiers[-1].predict(train_images)  # 获取预测结果\n",
    "    combined_output_train += alpha * predictions_train   # 计算组合输出\n",
    "    final_predictions_train = np.argmax(combined_output_train, axis=1)  # 获取组合预测的类别\n",
    "    true_label_train = np.argmax(train_labels, axis=1)    # 获取真实的类别\n",
    "    final_accuracy_train = np.mean(final_predictions_train == true_label_train) # 计算准确率\n",
    "    final_accuracy_train_list.append(final_accuracy_train)  # 记录训练准确率\n",
    "\n",
    "\n",
    "    print('------------------------------------------------')\n",
    "    print(f'训练集准确率: {train_accuracy[-1][-1]:.4f}')\n",
    "    print(f'测试集准确率: {test_accuracy[-1][-1]:.4f}')\n",
    "    print(f'Adaboost算法在训练集上的准确率: {final_accuracy_train:.4f}')\n",
    "    print(f'Adaboost算法在测试集上的准确率: {final_accuracy_test:.4f}')\n",
    "    print(alpha)\n",
    "print(alpha_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_classifier + 1), train_accuracy, label='Train Accuracy', marker='o')\n",
    "plt.plot(range(1, num_classifier + 1), test_accuracy, label='Test Accuracy', marker='o')\n",
    "plt.plot(range(1, num_classifier + 1), final_accuracy_test_list, label='Adaboost Accuracy in Test', marker='o')\n",
    "plt.plot(range(1, num_classifier + 1), final_accuracy_train_list, label='Adaboost Accuracy in Train', marker='o')\n",
    "plt.xlabel('Number of Classifiers')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Number of Classifiers')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "        train_images = idx2numpy.convert_from_file(r\"..\\train-images.idx3-ubyte\")\n",
    "        train_labels = idx2numpy.convert_from_file(r\"..\\train-labels.idx1-ubyte\")\n",
    "        test_images = idx2numpy.convert_from_file(r\"..\\t10k-images.idx3-ubyte\")\n",
    "        test_labels = idx2numpy.convert_from_file(r\"..\\t10k-labels.idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1.0\n",
      "3 2.718281828459045\n",
      "4 4.879108412035346\n",
      "5 7.38905609893065\n",
      "6 10.195312898128853\n",
      "7 13.262791735517345\n",
      "8 16.56604174441005\n",
      "9 20.085536923187668\n",
      "10 23.805698896394073\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "for i in range(2,11):\n",
    "    print(i,np.exp(math.log(i-1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
